#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ¼”ç¤ºæ¨¡å‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç›®æ ‡å˜é‡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app import create_app
from app.extensions import db
from app.models import FactorValues, MLModelDefinition
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import joblib
import os

def create_simple_demo_model():
    """åˆ›å»ºç®€åŒ–çš„æ¼”ç¤ºæ¨¡å‹"""
    app = create_app()
    
    with app.app_context():
        print("ğŸ¯ åˆ›å»ºç®€åŒ–çš„æ¼”ç¤ºæ¨¡å‹")
        print("=" * 60)
        
        try:
            # 1. è·å–å› å­æ•°æ®
            print("ğŸ“Š è·å–å› å­æ•°æ®...")
            factor_data = pd.read_sql('''
                SELECT ts_code, factor_id, factor_value
                FROM factor_values
                WHERE factor_id IN ('chip_concentration', 'money_flow_strength')
            ''', db.engine)
            
            print(f"   åŸå§‹æ•°æ®: {len(factor_data)} æ¡è®°å½•")
            
            # 2. åˆ›å»ºé€è§†è¡¨
            print("ğŸ”„ åˆ›å»ºç‰¹å¾çŸ©é˜µ...")
            feature_df = factor_data.pivot_table(
                index='ts_code',
                columns='factor_id',
                values='factor_value',
                aggfunc='first'
            ).reset_index()
            
            # åˆ é™¤ç¼ºå¤±å€¼
            feature_df = feature_df.dropna()
            print(f"   ç‰¹å¾çŸ©é˜µ: {len(feature_df)} è¡Œ Ã— {len(feature_df.columns)-1} åˆ—")
            
            if len(feature_df) < 50:
                print("âŒ æ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
                return
            
            # 3. åˆ›å»ºæ¨¡æ‹Ÿç›®æ ‡å˜é‡
            print("ğŸ² åˆ›å»ºæ¨¡æ‹Ÿç›®æ ‡å˜é‡...")
            np.random.seed(42)
            
            # åŸºäºå› å­å€¼åˆ›å»ºåˆç†çš„ç›®æ ‡å˜é‡
            X = feature_df[['chip_concentration', 'money_flow_strength']].values
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X)
            
            # åˆ›å»ºæœ‰æ„ä¹‰çš„ç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæœªæ¥æ”¶ç›Šç‡ï¼‰
            # ä½¿ç”¨å› å­çš„çº¿æ€§ç»„åˆåŠ ä¸Šå™ªå£°
            weights = np.array([0.3, 0.5])  # å› å­æƒé‡
            signal = np.dot(X_scaled, weights)
            noise = np.random.normal(0, 0.02, len(signal))  # 2%çš„å™ªå£°
            y = signal * 0.05 + noise  # ç¼©æ”¾åˆ°åˆç†çš„æ”¶ç›Šç‡èŒƒå›´
            
            print(f"   ç›®æ ‡å˜é‡èŒƒå›´: {y.min():.4f} è‡³ {y.max():.4f}")
            print(f"   ç›®æ ‡å˜é‡å‡å€¼: {y.mean():.4f}, æ ‡å‡†å·®: {y.std():.4f}")
            
            # 4. è®­ç»ƒæ¨¡å‹
            print("ğŸš€ è®­ç»ƒæ¨¡å‹...")
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )
            
            # åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹
            model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            
            # è®­ç»ƒ
            model.fit(X_train, y_train)
            
            # 5. è¯„ä¼°æ¨¡å‹
            print("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
            y_train_pred = model.predict(X_train)
            y_test_pred = model.predict(X_test)
            
            train_r2 = r2_score(y_train, y_train_pred)
            test_r2 = r2_score(y_test, y_test_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
            
            print(f"   è®­ç»ƒRÂ²: {train_r2:.4f}")
            print(f"   æµ‹è¯•RÂ²: {test_r2:.4f}")
            print(f"   è®­ç»ƒRMSE: {train_rmse:.4f}")
            print(f"   æµ‹è¯•RMSE: {test_rmse:.4f}")
            
            # 6. ä¿å­˜æ¨¡å‹
            print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
            model_dir = 'models'
            os.makedirs(model_dir, exist_ok=True)
            
            model_path = os.path.join(model_dir, 'simple_demo_model.pkl')
            scaler_path = os.path.join(model_dir, 'simple_demo_scaler.pkl')
            
            joblib.dump(model, model_path)
            joblib.dump(scaler, scaler_path)
            
            print(f"   æ¨¡å‹ä¿å­˜è‡³: {model_path}")
            print(f"   ç¼©æ”¾å™¨ä¿å­˜è‡³: {scaler_path}")
            
            # 7. åˆ›å»ºæ•°æ®åº“æ¨¡å‹å®šä¹‰
            print("ğŸ“ åˆ›å»ºæ•°æ®åº“æ¨¡å‹å®šä¹‰...")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = MLModelDefinition.query.filter_by(model_id='simple_demo_model').first()
            if existing:
                db.session.delete(existing)
                db.session.commit()
            
            # åˆ›å»ºæ–°å®šä¹‰
            model_def = MLModelDefinition(
                model_id='simple_demo_model',
                model_name='ç®€åŒ–æ¼”ç¤ºæ¨¡å‹',
                model_type='random_forest',
                factor_list=['chip_concentration', 'money_flow_strength'],
                target_type='simulated_return',
                model_params={
                    'n_estimators': 100,
                    'max_depth': 10,
                    'random_state': 42
                },
                training_config={
                    'test_size': 0.2,
                    'scaling_method': 'robust',
                    'use_simulated_target': True
                }
            )
            
            db.session.add(model_def)
            db.session.commit()
            
            print("âœ… ç®€åŒ–æ¼”ç¤ºæ¨¡å‹åˆ›å»ºå®Œæˆï¼")
            print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
            print(f"   æ¨¡å‹ID: simple_demo_model")
            print(f"   ç‰¹å¾æ•°é‡: 2 (chip_concentration, money_flow_strength)")
            print(f"   æ ·æœ¬æ•°é‡: {len(feature_df)}")
            print(f"   è®­ç»ƒæ ·æœ¬: {len(X_train)}")
            print(f"   æµ‹è¯•æ ·æœ¬: {len(X_test)}")
            print(f"   æ¨¡å‹æ€§èƒ½: RÂ² = {test_r2:.4f}")
            
            # 8. æ¼”ç¤ºé¢„æµ‹
            print("\nğŸ”® æ¼”ç¤ºé¢„æµ‹...")
            sample_indices = np.random.choice(len(X_test), min(5, len(X_test)), replace=False)
            
            for i, idx in enumerate(sample_indices):
                pred = y_test_pred[idx]
                actual = y_test[idx]
                print(f"   æ ·æœ¬{i+1}: é¢„æµ‹={pred:.4f}, å®é™…={actual:.4f}, è¯¯å·®={abs(pred-actual):.4f}")
            
            print("\nğŸ‰ ç°åœ¨æ‚¨å¯ä»¥åœ¨Webç•Œé¢ä¸­ä½¿ç”¨ 'simple_demo_model' è¿›è¡Œé¢„æµ‹äº†ï¼")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    create_simple_demo_model() 